{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cross_entropy20211017.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzFzHelrYI6klg1auJ2mmd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tangmaomao16/deep_learning_study/blob/main/cross_entropy20211017.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA0omERtqtR9"
      },
      "source": [
        "# Cross entropy交叉熵"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-LSCuU_rHG1"
      },
      "source": [
        "![cross entropy formula](https://i.stack.imgur.com/gNip2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIs1hJspjDHD"
      },
      "source": [
        "https://stackoverflow.com/questions/41990250/what-is-cross-entropy\n",
        "\n",
        "这篇论坛帖子，讲述了机器学习中的cross entropy的概念。\n",
        "\n",
        "cross entropy交叉熵。\n",
        "\n",
        "有一个真实数据统计出来的概率vector，通常把它记为p，例如p = [0,1,0]，它表示在真实数据中，class A的频率为0，class B的频率为1，class C的频率为0。\n",
        "\n",
        "有一个机器学习得到的对某个输入数据预测，这个输入数据属于哪一个class的概率vector，通常把它记为q，例如q = [0.228, 0.619, 0.153]。\n",
        "\n",
        "cross entropy的计算式为，p中的class的概率值，乘以，q中的相应class的概率值的对数值(对数运算的底数通常为e，也可以是2；由于概率值一般是0到1的数，所以对数运算结果是0到负无穷的负数)，把所有class的这些乘积值都加起来，最后乘以-1，得到的结果是0到正无穷的正数。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-wyq9MOkYtU"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "p = np.array([0, 1, 0])\n",
        "q = np.array([0.001, 0.001, 0.998])\n",
        "print(-np.sum(p * np.log(q)))\n",
        "# 6.907755278982137"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jBB6QO_oUSM"
      },
      "source": [
        "在某一个class的位置中，-p*log(q)，表示真实数据属于这一个class的概率，与预测结果中输入数据属于这一个class的概率，这两个概率值之间的距离或是error。\n",
        "\n",
        "当p接近1，q接近0时（ log(q)的绝对值很大 ），这个式子算出来的值是正数，而且是一个很大的正数，表明它们的距离很远。\n",
        "\n",
        "当p接近0，q接近1时（ log(q)也接近0 ），这个式子算出来的值接近0。尽管这一个class位置上算出来的距离值很小，但是由于概率vector中各个元素加起来为1，所以必然会有其它class位置上的q接近0（ log(q)的绝对值很大 ），那么算出来cross entropy的值也会是一个很大的正数。\n",
        "\n",
        "从这里还可以看出，classify分类问题，至少是二元分类问题，也就是至少有2种class，不存在只有1种class的分类问题。"
      ]
    }
  ]
}